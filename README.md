


> Written with [StackEdit](https://stackedit.io/).

## Coursera: Mathematics or Machine Learning: Linear Algebra

### "WEEK 1"
A good book for Linear algebra: 
> [http://math.mit.edu/~gs/linearalgebra/](http://math.mit.edu/~gs/linearalgebra/)

### Simplest form of a linear algebra equation:
$\left\{\begin{matrix}
2a + 3b = 8 &  & \\ 
10a + 1b = 36 &  & 
\end{matrix}\right.$

In this equation we have the coefficients:$\begin{bmatrix}
2 & 3\\ 
10 & 1
\end{bmatrix}$ which related variables: $\begin{bmatrix}
a\\ 
b
\end{bmatrix} to$ the outputs: $\begin{bmatrix}
8\\ 
13
\end{bmatrix}$.
### Normal Distribution (Gaussian):
> $f(x) = \frac{1}{\sigma \sqrt{2\pi }} e^{\frac{-(x-\mu )^2}{2\sigma ^2}}$

Where $\sigma$ is the **standard deviation** , and $\mu$ is the **mean.**

#### Residual: 
The difference between the measured data, and the modelled predictions.

### Vectors and Operations on Vectors:

- $\vec{a} + \vec{b} = \vec{b} + \vec{a}$

vectors can be interchangably summed up. order doesn't matter. 

- we can multiply a vector into a constant value, like this: $m\vec{a}$ and we simply multiply each element of $\vec{a}$ to  m.  
-   $(\vec{a}+\vec{b})+\vec{c} = \vec{a} + (\vec{b} + \vec{c})$  

This is called __Associativity.__

      


